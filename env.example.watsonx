# ====================================================================
# LocalGPT Watson X Configuration Example
# ====================================================================
# This file shows how to configure LocalGPT to use IBM Watson X AI
# with Granite models instead of local Ollama.
#
# Copy this file to .env and fill in your credentials:
#   cp .env.example.watsonx .env
# ====================================================================

# LLM Backend Selection
# Options: "ollama" (default) or "watsonx"
LLM_BACKEND=watsonx

# ====================================================================
# Watson X Credentials
# ====================================================================
# Get these from your IBM Cloud Watson X project:
# 1. Go to https://cloud.ibm.com/
# 2. Navigate to Watson X AI service
# 3. Create or select a project
# 4. Get API key from IBM Cloud IAM
# 5. Copy project ID from project settings

# Your IBM Cloud API key
WATSONX_API_KEY=your_api_key_here

# Your Watson X project ID
WATSONX_PROJECT_ID=your_project_id_here

# Watson X service URL (default: us-south region)
# Options:
#   - https://us-south.ml.cloud.ibm.com (US South)
#   - https://eu-de.ml.cloud.ibm.com (Frankfurt)
#   - https://eu-gb.ml.cloud.ibm.com (London)
#   - https://jp-tok.ml.cloud.ibm.com (Tokyo)
WATSONX_URL=https://us-south.ml.cloud.ibm.com

# ====================================================================
# Model Configuration
# ====================================================================
# Granite models available on Watson X

# Main generation model for answering queries
# Options:
#   - ibm/granite-13b-chat-v2 (recommended for chat)
#   - ibm/granite-13b-instruct-v2 (for instructions)
#   - ibm/granite-20b-multilingual (for multilingual)
#   - ibm/granite-3b-code-instruct (for code)
WATSONX_GENERATION_MODEL=ibm/granite-13b-chat-v2

# Lightweight model for enrichment and routing
# Use a smaller model for better performance on simple tasks
WATSONX_ENRICHMENT_MODEL=ibm/granite-8b-japanese

# ====================================================================
# Optional: Ollama Configuration (fallback)
# ====================================================================
# These settings are used if LLM_BACKEND=ollama

OLLAMA_HOST=http://localhost:11434
